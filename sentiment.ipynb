{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuDHXuV7HM9S"
   },
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "UzSzsOiYKYKf",
    "outputId": "832c74ac-cc76-4aff-ac41-65808350cd27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "3PemhvgMKuxq",
    "outputId": "b140a8d7-ec0f-4777-9909-64649f815eea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "hYeOtfGlBR6t",
    "outputId": "7da807b7-6226-4316-fe33-9a58bcdededd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtm8_yhrpPEX"
   },
   "source": [
    "### Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZULQq8EsOAqv"
   },
   "outputs": [],
   "source": [
    "# word frequency\n",
    "from nltk import ngrams, FreqDist\n",
    "\n",
    "def word_freq(words_list):\n",
    "  counts = {}\n",
    "  for word in words_list:\n",
    "    if word in counts.keys():\n",
    "      counts[word] += 1\n",
    "    else:\n",
    "      counts[word] = 1\n",
    "  return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhffZ9LXQWlR"
   },
   "outputs": [],
   "source": [
    "def lower_case(word_list):\n",
    "  word_list = [word.lower() for word in word_list]\n",
    "  return word_list\n",
    "\n",
    "assert ['my', 'university'] == lower_case(['My', 'University'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "4jn8Ofu6B798",
    "outputId": "7891fb40-4f2b-4d47-8413-dd29a675a2d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49815\n"
     ]
    }
   ],
   "source": [
    "# Vocab is all Unique words in the corpus\n",
    "def Vocab(corpus_words):\n",
    "  corpus_words = lower_case(corpus_words)\n",
    "  V_ = word_freq(corpus_words).keys()\n",
    "  V_ = list(V_)\n",
    "  return V_\n",
    "\n",
    "print(len(Vocab(brown.words())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6e7AltkEpI6M"
   },
   "source": [
    "### Convert words into sparse vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCk_gKhfCamw"
   },
   "outputs": [],
   "source": [
    "# this is heavy process and it has lots of zeros \n",
    "def sparse_represent(sentence, V):\n",
    "  sparse_representation = [0]*len(V)\n",
    "  i = 0\n",
    "  for word in sentence:\n",
    "    for i in np.arange(len(V)):\n",
    "      if word == V[i]:\n",
    "        sparse_representation[i] = 1\n",
    "  return sparse_representation\n",
    "\n",
    "V = Vocab(brown.words())\n",
    "test_sentence = ['i', 'am', 'happy']\n",
    "assert len(sparse_represent(test_sentence, V)) == 49815"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yPOoZyopDqT"
   },
   "source": [
    "### Using nltk Sentiment analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "Pxw37JsCLK_k",
    "outputId": "97ac9295-5301-462c-ebd6-53ef9d6d5e1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
      "{'neg': 0.0, 'neu': 0.425, 'pos': 0.575, 'compound': 0.8877}\n",
      "{'neg': 0.206, 'neu': 0.351, 'pos': 0.443, 'compound': 0.5719}\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# sentiment_analyzer is used for some basic sentiment analysis tasks\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "a1 = 'This was the best, most awesome movie EVER MADE!!!'\n",
    "print(sid.polarity_scores(a1))\n",
    "a2 = 'I hate this movie, but I am happy'\n",
    "print(sid.polarity_scores(a2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CDSzavNSpXXc"
   },
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "o_UB5R9eolxW",
    "outputId": "711855ef-50c2-43b8-b484-7ee2bdba95ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "['i', 'me', 'my', 'myself', 'we']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "print(stopwords.words('english')[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WslaPiBzpber"
   },
   "source": [
    "### Sentiment Analysis on tweets using logistic regression and NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "Sav5yQ0Co8io",
    "outputId": "511a4fd6-1e11-4a14-effd-d19537878850"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "#nltk.download('twitter_samples')\n",
    "\n",
    "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "neg_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ViIN6L3Ax4f7"
   },
   "outputs": [],
   "source": [
    "val_pos = pos_tweets[4000:]\n",
    "train_pos = pos_tweets[:4000]\n",
    "val_neg = neg_tweets[4000:]\n",
    "train_neg = neg_tweets[:4000] \n",
    "train_x = train_pos + train_neg \n",
    "val_x  = val_pos + val_neg\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "val_y  = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAZZhFkTzW4U"
   },
   "source": [
    "Process tweets and remove all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m635towEyJHM"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "# helper function to clean tweets\n",
    "def process_sentence(sentence):\n",
    "    # Remove HTML special entities (e.g. &amp;)\n",
    "    sentence = re.sub(r'\\&\\w*;', '', sentence)\n",
    "    #Convert @username to AT_USER\n",
    "    sentence = re.sub('@[^\\s]+','',sentence)\n",
    "    # Remove tickers\n",
    "    sentence = re.sub(r'\\$\\w*', '', sentence)\n",
    "    # To lowercase\n",
    "    sentence = sentence.lower()\n",
    "    # Remove hyperlinks\n",
    "    sentence = re.sub(r'https?:\\/\\/.*\\/\\w*', '', sentence)\n",
    "    # Remove hashtags\n",
    "    sentence = re.sub(r'#\\w*', '', sentence)\n",
    "    # Remove Punctuation and split 's, 't, 've with a space for filter\n",
    "    sentence = re.sub(r'[' + string.punctuation.replace('@', '') + ']+', ' ', sentence)\n",
    "    # Remove words with 2 or fewer letters\n",
    "    sentence = re.sub(r'\\b\\w{1,2}\\b', '', sentence)\n",
    "    # Remove whitespace (including new line characters)\n",
    "    sentence = re.sub(r'\\s\\s+', ' ', sentence)\n",
    "    # Remove single space remaining at the front of the tweet.\n",
    "    sentence = sentence.lstrip(' ') \n",
    "    # Remove characters beyond Basic Multilingual Plane (BMP) of Unicode:\n",
    "    sentence = ''.join(c for c in sentence if c <= '\\uFFFF') \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "Buy9Kpx2yK-e",
    "outputId": "f3c330f3-9c75-405c-d8c4-71bf88a5e813"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'hey james how odd please call our contact centre 02392441234 and will able assist you many thanks '"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_sentence(train_x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "NlsADjq50enO",
    "outputId": "4a949612-a7fa-4237-994d-0b3512b13149"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 2000)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x), len(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4Q19YHH5Nxb"
   },
   "outputs": [],
   "source": [
    "def features_of_sentences(sentences):\n",
    "  features = []\n",
    "  for sentence in sentences:\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    pos = sid.polarity_scores(sentence)['pos']\n",
    "    neg = sid.polarity_scores(sentence)['neg']\n",
    "    f = [pos, neg]\n",
    "    features.append(f)\n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LkdvxDiW5Qdk"
   },
   "outputs": [],
   "source": [
    "train_x_features = features_of_sentences(train_x)\n",
    "val_x_features = features_of_sentences(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_lWZozVEDD1"
   },
   "outputs": [],
   "source": [
    "val_x_features = np.array(val_x_features)\n",
    "train_x_features = np.array(train_x_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "6b8hasRQ9r49",
    "outputId": "5a50fffb-3b79-48a1-8027-0208f61a3fea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(random_state=0)\n",
    "logreg.fit(train_x_features, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "i-nyVvD1CTrL",
    "outputId": "bbd6be23-3b51-4869-e425-ab12eebd6ff6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8785\n"
     ]
    }
   ],
   "source": [
    "print(logreg.score(val_x_features, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "nDYQRSBBCkQl",
    "outputId": "7c609153-0586-41cc-a6d3-0dff6ede2c22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love machine learning  :  1.0  with probablities [0.023144 0.976856]\n",
      "I like machine learning but it's stupid  :  0.0  with probablities [0.97238472 0.02761528]\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\"I love machine learning\", \"I like machine learning but it's stupid\"]\n",
    "test_sentences_features = features_of_sentences(test_sentences)\n",
    "prediction = logreg.predict(test_sentences_features)\n",
    "prediction_proba = logreg.predict_proba(test_sentences_features)\n",
    "print(test_sentences[0],\" : \",  prediction[0], \" with probablities\", prediction_proba[0])\n",
    "print(test_sentences[1],\" : \",  prediction[1], \" with probablities\", prediction_proba[1])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
